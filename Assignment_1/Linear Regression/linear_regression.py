# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0KWvrH2Lk_q8C2AnYZeOXpt0sH8J6ld
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle

"""# Read and Shuffle"""

df = pd.read_csv("car_data.csv")
df.insert(1,"ones",1)
df.head()

df = (shuffle(df))
df.head()

"""# Select Features"""

numerical_columns = (df[df.columns[10:]]).drop(columns=["fuelsystem", "enginetype", "cylindernumber"])
plt.figure(figsize=(20,10))
sns.heatmap(numerical_columns.corr(),annot=True)
plt.show()

# Price on y-axis
numerical_columns.drop(columns=['price'], inplace=True)
fig = plt.figure(figsize=(30,30))
for index, col in enumerate(numerical_columns.columns):
  plt.subplot(6,3,index+1)
  plt.scatter(numerical_columns[col], df[df.columns[-1]])
  plt.title(col)

x = df[['ones', 'carwidth', 'curbweight', 'enginesize', 'horsepower']]
size=int(len(x) * 0.8)
y = df['price']
x.head()

y.head()

"""# Normalize"""

x_min = x.min(axis=0)
x_max = x.max(axis=0)

normalized_X = (x - x_min) / (x_max - x_min)
normalized_X['ones'] = x['ones']
normalized_X.head()

"""# Splitting"""

col = normalized_X.shape[0]
x_train = normalized_X[:int(col*0.8)]
x_test = normalized_X[int(col*0.8):]
y_train = y[:int(col*0.8)]
y_test = y[int(col*0.8):]

"""# Model"""

theta = np.zeros(np.size(x, 1))
def hypothesis(xi, thetai):
  return np.dot(xi, thetai)

def cost(hypothesis, y_actual, m):
  diff = hypothesis - y_actual
  cost = np.dot(diff.T, diff) / (2 * m)
  return cost

def gradient_descent(learning_rate, m, xi, yi, theta, iterations):
  prev_c = []
  prev_t = []
  for i in range(iterations):
    h = hypothesis(xi, theta)
    diff = np.dot(xi.T, (h - yi))
    theta = theta - ((learning_rate / m) * diff)
    prev_t.append(theta)
    j = cost(h, yi, m)
    prev_c.append(j)
  return theta, prev_t, prev_c

def predict(xi, yi, theta):
  y_predict = hypothesis(xi, theta)
  return y_predict

learning_rate = [0.0008, 0.008, 0.01, 0.08, 0.1, 0.5, 0.8]
iterations = 1000
fig = plt.figure(figsize=(30,35))

for lr in range(len(learning_rate)):
  print('Learning rate =', learning_rate[lr])
  (theta, prev_t, prev_c ) = gradient_descent(learning_rate[lr], size, x_train, y_train,theta, iterations)
  
  for i in range(len(theta)):
    if i == 0:
      print('\tb', '= ',theta[i])
    else:
      print('\tTheta', i, '= ',theta[i])

  y_predict = predict(x_test, y_test, theta)
  MSE = cost(y_predict, y_test, size)
  print('Error for test data =',MSE,'\n')

  plt.subplot(6,4,lr+1)
  plt.plot(prev_c )
  plt.title(label=('Change of theta with alpha =', learning_rate[lr]))
  plt.xlabel('Iterations')
  plt.ylabel('Cost')
  #plt.show()
  #plt.close()

